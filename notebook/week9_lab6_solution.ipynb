{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76ca574",
   "metadata": {},
   "source": [
    "# CS4243 - Lab Session 6\n",
    "\n",
    "Computer Vision & Pattern Recognition\n",
    "\n",
    "Week 9, Mon 17 Oct, AY 2023/24\n",
    "\n",
    "Author: Dr. Amirhassan MONAJEMI. Modified by: Lingdong KONG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded9c9a",
   "metadata": {},
   "source": [
    "## Welcome! ðŸ‘‹\n",
    "\n",
    "This notebook contains the tutorials for the `sixth` lab sessions. The following materials are covered:\n",
    "\n",
    "- Part 1: Optical Flow\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Part 1: Optical Flow\n",
    "\n",
    "#### Goal of this section:\n",
    "- Learn how to track feature points using the *Lucas-Kanade algorithm* in the `lucas_kanade_point_tracking` function.\n",
    "- Learn how to draw the tracked points and the motion vectors on each frame.\n",
    "\n",
    "    Note:\n",
    "    \n",
    "    - The tracking continues until you press the `'Esc'` key to exit the application.\n",
    "    - Make sure that the path to your video file is correct. \n",
    "\n",
    "  Extra:\n",
    "    \n",
    "    - You can configure the code to use your camera as the video source.\n",
    "    - You might need to adjust the parameters for good feature point detection (e.g., `cv2.goodFeaturesToTrack`) to suit your specific tracking needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfd455",
   "metadata": {},
   "source": [
    "We start by capturing video frames from a video file.\n",
    "\n",
    "\n",
    "#### Videos for testing: \n",
    "- Example 1: `vtest.avi`\n",
    "- Example 2: `10142.mp4`\n",
    "- Example 3: `10231.mp4`\n",
    "- Example 4: `10236.mp4`\n",
    "\n",
    "You can download these videos from Canvas: CS4243 -> Files -> Python_notebooks -> set5 -> `OF_samples.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71b050",
   "metadata": {},
   "source": [
    "#### Procedures:\n",
    "1. Run the code using a few test videos (as listed above).\n",
    "2. Understand the role of the parameters and change them accordingly to comments.\n",
    "3. Try to make it better again based on the comments.\n",
    "4. Answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca256cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5a42e",
   "metadata": {},
   "source": [
    "**Main Function** (`lucas_kanade_point_tracking`)\n",
    "\n",
    "- Input parameters:\n",
    "    - Two frames, `prev_frame` and `curr_frame`, and an array of `prev_points` coordinations.\n",
    "- Returns:\n",
    "    - The updated `prev_points` and the new `next_points` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb12451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lucas_kanade_point_tracking(prev_frame, curr_frame, prev_points):\n",
    "    \n",
    "    lk_params = dict(\n",
    "        winSize=(15, 15),\n",
    "        maxLevel=2,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "    )\n",
    "\n",
    "    # Step 1: calculate optical flow using Lucas-Kanade method\n",
    "    next_points, status, _ = cv2.calcOpticalFlowPyrLK(\n",
    "        prev_frame, curr_frame, prev_points, None, **lk_params\n",
    "    )\n",
    "    status = status.ravel()\n",
    "    \n",
    "    # Step 2: filter out points with status = 1 (successfully tracked)\n",
    "    prev_points = prev_points[status == 1]\n",
    "    next_points = next_points[status == 1]\n",
    "\n",
    "    return prev_points, next_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d7125",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q1** - What are the types and shapes of those parameters?\n",
    "> - **Answer:** \n",
    "    > - `prev_frame` and `curr_frame`: These are images, typically represented as 2D arrays for grayscale images or 3D arrays for colored images. Their types would be numpy arrays. The shape of these frames would be `(height, width)` for grayscale or `(height, width, channels)` for colored images.\n",
    "    > - `prev_points`: This is an array of coordinates. Its type would be a numpy array, and its shape would be `(n, 2)`, where `n` is the number of points and the 2 represents the x and y coordinates.\n",
    "\n",
    "> **Q2** - What are the shapes of those returned arrays? Any changes in `prev_points`?\n",
    "> - **Answer:** \n",
    "    > - Both `prev_points` and `next_points` will have the shape `(m, 2)`, where `m` is the number of successfully tracked points (which could be less than or equal to `n`, the original number of points in `prev_points`).\n",
    "    > - Yes, `prev_points` might be changed in this function. Only those points which were successfully tracked (i.e., where the status is 1) are retained.\n",
    "\n",
    "> **Q3** - What are the parameters for the Lucas-Kanade optical flow method? what do they do?\n",
    "> - **Answer:**\n",
    "    > - `winSize`: Size of the search window at each pyramid level. In this function, it's set to (15, 15). Changing the window size will affect the accuracy and computation time. A larger window might capture more evident motion but at a higher computation cost.\n",
    "    > - `maxLevel`: The 0-based maximal pyramid level number. In this function, it's set to 2. This means that there are three levels in total: level `0` (original image), level `1`, and level `2`. Adjusting the number of levels affects the algorithm's ability to capture motion at various scales. A value of 0 means only the original scale, while a higher value allows for capturing motion at reduced resolutions.\n",
    "    > - `criteria`: This sets the termination criteria of the iterative search algorithm. The function uses the combination of a maximum number of iterations (`cv2.TERM_CRITERIA_COUNT`) and a specified amount of epsilon (`cv2.TERM_CRITERIA_EPS`), which determines the minimal desired accuracy.\n",
    "> - **Extra:**\n",
    "    > You can try changing `winSize` to (50, 50) and `maxLevel` to 0 and 4 to see how they affect the tracking performance.\n",
    " \n",
    "> **Q4** - What are the criteria? how to finish an iterative algorithm?\n",
    "> - **Answer:** \n",
    "    > - The criteria are set using a combination of two factors:\n",
    "        > 1. The maximum number of the iteration (set to 10 in this function).\n",
    "        > 2. Epsilon accuracy (set to 0.03 here).\n",
    "    > - The iterative algorithm stops when either of the conditions is met: it either reaches the specified number of iterations or achieves the desired accuracy.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- Source 1: https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html\n",
    "- Source 2: https://www.geeksforgeeks.org/python-opencv-optical-flow-with-lucas-kanade-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5415ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the video\n",
    "cap = cv2.VideoCapture('OF_samples/10231.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311977a",
   "metadata": {},
   "source": [
    "Learn more about the `VideoCapture` function of OpenCV. \n",
    "\n",
    "**Questions:**\n",
    "\n",
    "> **Q5** - Do we need other parameters here?\n",
    "> - **Answer:**\n",
    "    > - The `cv2.VideoCapture()` function primarily takes one argument, which can be either the name of a video file (like in the above function) or an integer number representing the ID of the camera device you want to use.\n",
    "    > - So, in the provided function, only the path to the video file is needed.\n",
    "    > - However, if you want to access a different camera or live stream, you might use a different argument, typically an integer (like 0 for the default camera). In some situations, you might also want to set other properties of the `VideoCapture` object using the `set()` method, but these are not parameters of the `VideoCapture` function itself.\n",
    "\n",
    "> **Q6** - How can we read a live video stream?\n",
    "> - **Answer:**\n",
    "    > - To read a live video stream using OpenCV's `VideoCapture` function, you would typically pass an integer representing the camera's ID.\n",
    "    > - For most systems, `0` would be the default built-in camera (like a laptop's webcam). `1` would be a secondary camera if connected, and so on.\n",
    "    > -  So, to capture video from the default camera, you would initialize the `VideoCapture` object like this:\n",
    "    > ```python\n",
    "    > cap = cv2.VideoCapture(0)\n",
    "    > ```\n",
    "    > - After initializing, you can then read frames in a loop using the `read()` method until you decide to break the loop or the stream ends.\n",
    "\n",
    "\n",
    "**References:**\n",
    "- Source 3: https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c2c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first frame\n",
    "ret, prev_frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47962b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8226b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "print(prev_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575e036",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q7** - What does the `ret` parameter do here?\n",
    "> - **Answer:**\n",
    "    > - The `ret` is a boolean variable that returns `True` if the frame is successfully grabbed (or read) and `False` otherwise. \n",
    "    > - When working with video files or live video streams in OpenCV, it's common to use `ret` to check if a frame has been successfully retrieved. \n",
    "    > - If `ret` is `False`, it often indicates that there are no more frames to read (in case of a video file) or there was some issue in capturing the frame (in case of a live stream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c136c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the frame to gray level\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3b82a",
   "metadata": {},
   "source": [
    "Define initial points to track (e.g., using `cv2.goodFeaturesToTrack`)\n",
    "- We can have any function which returns an array of the [x, y] coordination pairs here.\n",
    "\n",
    "Input Parameters:\n",
    "- `prev_gray`: the frame that used to extract feature points or anchor points from.\n",
    "- `maxCorners`: the maximum number of points to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86399a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_points = cv2.goodFeaturesToTrack(\n",
    "    prev_gray,\n",
    "    maxCorners=100,\n",
    "    qualityLevel=0.3,\n",
    "    minDistance=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c91c3c",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q8** - What if changing `maxCorners` to `4`?\n",
    "> - **Answer:** \n",
    "    > - The `maxCorners` parameter determines the maximum number of corners or feature points to return.\n",
    "    > - If set to 4, the `cv2.goodFeaturesToTrack()` function will return at most 4 of the strongest corners (or feature points) it can find in the `prev_gray` image. \n",
    "    > - This means that even if there are more potential corners available, only the top 4 (based on the given qualityLevel criteria) will be selected.\n",
    "\n",
    "> **Q9** - What if changing `qualityLevel` to `0.9` and `0.01`?\n",
    "> - **Answer:** \n",
    "    > - The `qualityLevel` parameter is a multiplier for the minimum eigenvalue of a 2x2 normal matrix of optical flow equations, the smallest eigenvalue of which is used to sort the corners. In simpler terms, it sets the threshold for the quality of the corners.\n",
    "    > - If set to 0.9: The function will be stricter in determining which corners are considered \"good\". It will only select corners that have a quality score greater than 90% of the highest quality corner detected. As a result, you might end up with fewer corners than the specified maxCorners value, unless there are many very high-quality corners.\n",
    "    > - If set to 0.01: The function will be more lenient, and you'll get many more corners, potentially up to your `maxCorners` limit, but many of these corners might not be very prominent or distinct.\n",
    "\n",
    "> **Q10** - The `minDistance` parameter is the minimum distance between detected feature points in pixel. What if changing it to `1` and `50`?\n",
    "> - **Answer:** \n",
    "    > - The `minDistance` parameter defines the minimum Euclidean distance between the returned corners.\n",
    "    > - If set to 1: Detected corners can be very close to each other, with a minimum distance of 1 pixel between them. This might result in clusters of points in regions with high texture.\n",
    "    > - If set to 50: Detected corners will be at least 50 pixels apart from each other. This ensures that the corners are well distributed over the image, but in small or tightly textured regions, you might miss out on potential feature points because of this larger distance constraint.\n",
    "\n",
    "**References:**\n",
    "- Source 4: https://docs.opencv.org/3.4/d4/d8c/tutorial_py_shi_tomasi.html \n",
    "- Source 5: https://theailearner.com/tag/cv2-goodfeaturestotrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd70f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    ret, curr_frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # track points using Lucas-Kanade method\n",
    "    prev_points, next_points = lucas_kanade_point_tracking(prev_gray, curr_gray, prev_points)\n",
    "\n",
    "    # draw tracks on the current frame\n",
    "    for i, (prev, next) in enumerate(zip(prev_points, next_points)):\n",
    "        x1, y1 = prev.ravel()\n",
    "        x2, y2 = next.ravel()\n",
    "        cv2.line(curr_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        cv2.circle(curr_frame, (int(x2), int(y2)), 5, (0, 255, 0), -1) \n",
    "        \n",
    "    cv2.namedWindow(\"Optical Flow\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('Optical Flow', curr_frame)\n",
    "    \n",
    "    # press 'Esc' to exit\n",
    "    if cv2.waitKey(0) & 0xFF == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27943e00",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q11** - How can we break the `while` loop? why ended with `0xFF`?\n",
    "> - **Answer:** \n",
    "    > - There are two conditions in the function that can break the `while` loop:\n",
    "    > 1. When the `ret` is False, which indicates that there is no more frame to read from the video or there was an issue in capturing the frame. This is checked by the condition:\n",
    "    > ```python\n",
    "    > if not ret: break\n",
    "    > ```\n",
    "    > 2. When the user presses the 'Esc' key. This is checked by the condition:\n",
    "    > ```python\n",
    "    > if cv2.waitKey(0) & 0xFF == 27: break\n",
    "    > ```\n",
    "    > - The `cv2.waitKey(0)` function waits indefinitely for a key event. The value returned by this function is a 32-bit integer. The & `0xFF` operation is a bitwise `AND` that is used to mask off the upper 24 bits of the integer, leaving only the lower 8 bits. \n",
    "    > - The ASCII value for the 'Esc' key is `27`, so the condition checks if the lower 8 bits of the returned integer equal `27`, meaning the 'Esc' key was pressed. The reason for the bitwise `AND` with `0xFF` is to handle compatibility between 32-bit and 64-bit systems, ensuring consistent behavior across different platforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0677d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987485c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8a572",
   "metadata": {},
   "source": [
    "## Review:\n",
    "\n",
    "The `cv2.calcOpticalFlowPyrLK()` function in OpenCV documentation.\n",
    "\n",
    "**Ref:** https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html?ref=nanonets.com#calcopticalflowpyrlk\n",
    "\n",
    "<br>\n",
    "\n",
    "**Previous implementation:**\n",
    "```python\n",
    "def lucas_kanade_point_tracking(prev_frame, curr_frame, prev_points):\n",
    "    \n",
    "    lk_params = dict(\n",
    "        winSize=(15, 15),\n",
    "        maxLevel=2,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "    )\n",
    "\n",
    "    # Step 1: calculate optical flow using Lucas-Kanade method\n",
    "    next_points, status, _ = cv2.calcOpticalFlowPyrLK(\n",
    "        prev_frame, curr_frame, prev_points, None, **lk_params\n",
    "    )\n",
    "    status = status.ravel()\n",
    "    \n",
    "    # Step 2: filter out points with status = 1 (successfully tracked)\n",
    "    prev_points = prev_points[status == 1]\n",
    "    next_points = next_points[status == 1]\n",
    "\n",
    "    return prev_points, next_points\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "> **`calcOpticalFlowPyrLK`**\n",
    "\n",
    "> Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.\n",
    "\n",
    "> Parameters:\t\n",
    "\n",
    "> - `prevImg` â€“ first 8-bit input image or pyramid constructed by `buildOpticalFlowPyramid()`.\n",
    "\n",
    "> - `nextImg` â€“ second input image or pyramid of the same size and the same type as `prevImg`.\n",
    "\n",
    "> - `prevPts` â€“ vector of 2D points for which the flow needs to be found; point coordinates must be single-precision floating-point numbers.\n",
    "\n",
    "> - `nextPts` â€“ output vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when `OPTFLOW_USE_INITIAL_FLOW` flag is passed, the vector must have the same size as in the input.\n",
    "\n",
    "> - `status` â€“ output status vector (of unsigned chars); each element of the vector is set to `1` if the flow for the corresponding features has been found, otherwise, it is set to `0`.\n",
    "\n",
    "> - `err` â€“ output vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasnâ€™t found then the error is not defined (use the status parameter to find such cases).\n",
    "\n",
    "> - `winSize` â€“ size of the search window at each pyramid level.\n",
    "\n",
    "> - `maxLevel` â€“ 0-based maximal pyramid level number; if set to `0`, pyramids are not used (single level), if set to `1`, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than `maxLevel`.\n",
    "\n",
    "> - `criteria` â€“ parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations `criteria.maxCount` or when the search window moves by less than `criteria.epsilon`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab7420",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next:\n",
    "\n",
    "We have seen an example using video `10231.mp4`. You might want to explore other videos in this tutorial:\n",
    "\n",
    "- Example 1: `vtest.avi`\n",
    "- Example 2: `10142.mp4`\n",
    "- Example 3: `10231.mp4` (Completed)\n",
    "- Example 4: `10236.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e0c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cd818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d93cfd",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ Congratulations! You have finished this lab tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
