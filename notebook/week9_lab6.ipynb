{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76ca574",
   "metadata": {},
   "source": [
    "# CS4243 - Lab Session 6\n",
    "\n",
    "Computer Vision & Pattern Recognition\n",
    "\n",
    "Week 9, Mon 17 Oct, AY 2023/24\n",
    "\n",
    "Author: Dr. Amirhassan MONAJEMI. Modified by: Lingdong KONG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded9c9a",
   "metadata": {},
   "source": [
    "## Welcome! ðŸ‘‹\n",
    "\n",
    "This notebook contains the tutorials for the `sixth` lab sessions. The following materials are covered:\n",
    "\n",
    "- Part 1: Optical Flow\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Part 1: Optical Flow\n",
    "\n",
    "#### Goal of this section:\n",
    "- Learn how to track feature points using the *Lucas-Kanade algorithm* in the `lucas_kanade_point_tracking` function.\n",
    "- Learn how to draw the tracked points and the motion vectors on each frame.\n",
    "\n",
    "    Note:\n",
    "    \n",
    "    - The tracking continues until you press the `'Esc'` key to exit the application.\n",
    "    - Make sure that the path to your video file is correct. \n",
    "\n",
    "  Extra:\n",
    "    \n",
    "    - You can configure the code to use your camera as the video source.\n",
    "    - You might need to adjust the parameters for good feature point detection (e.g., `cv2.goodFeaturesToTrack`) to suit your specific tracking needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfd455",
   "metadata": {},
   "source": [
    "We start by capturing video frames from a video file.\n",
    "\n",
    "\n",
    "#### Videos for testing: \n",
    "- Example 1: `vtest.avi`\n",
    "- Example 2: `10142.mp4`\n",
    "- Example 3: `10231.mp4`\n",
    "- Example 4: `10236.mp4`\n",
    "\n",
    "You can download these videos from Canvas: CS4243 -> Files -> Python_notebooks -> set5 -> `OF_samples.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71b050",
   "metadata": {},
   "source": [
    "#### Procedures:\n",
    "1. Run the code using a few test videos (as listed above).\n",
    "2. Understand the role of the parameters and change them accordingly to comments.\n",
    "3. Try to make it better again based on the comments.\n",
    "4. Answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca256cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d550f0",
   "metadata": {},
   "source": [
    "**Main Function** (`lucas_kanade_point_tracking`)\n",
    "\n",
    "- Input parameters:\n",
    "    - Two frames, `prev_frame` and `curr_frame`, and an array of `prev_points` coordinations.\n",
    "- Returns:\n",
    "    - The updated `prev_points` and the new `next_points` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb12451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lucas_kanade_point_tracking(prev_frame, curr_frame, prev_points):\n",
    "    \n",
    "    lk_params = dict(\n",
    "        winSize=(15, 15),\n",
    "        maxLevel=2,\n",
    "        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "    )\n",
    "\n",
    "    # Step 1: calculate optical flow using Lucas-Kanade method\n",
    "    next_points, status, _ = cv2.calcOpticalFlowPyrLK(\n",
    "        prev_frame, curr_frame, prev_points, None, **lk_params\n",
    "    )\n",
    "    status = status.ravel()\n",
    "    \n",
    "    # Step 2: filter out points with status = 1 (successfully tracked)\n",
    "    prev_points = prev_points[status == 1]\n",
    "    next_points = next_points[status == 1]\n",
    "\n",
    "    return prev_points, next_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb85473",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q1** - What are the types and shapes of those parameters?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "> **Q2** - What are the shapes of those returned arrays? Any changes in `prev_points`?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "> **Q3** - What are the parameters for the Lucas-Kanade optical flow method? what do they do?\n",
    "> - **Answer:**\n",
    "    \n",
    " \n",
    "> **Q4** - What are the criteria? how to finish an iterative algorithm?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "**References:**\n",
    "\n",
    "- Source 1: https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html\n",
    "- Source 2: https://www.geeksforgeeks.org/python-opencv-optical-flow-with-lucas-kanade-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the video\n",
    "cap = cv2.VideoCapture('OF_samples/10231.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192eb71",
   "metadata": {},
   "source": [
    "Learn more about the `VideoCapture` function of OpenCV. \n",
    "\n",
    "**Questions:**\n",
    "\n",
    "> **Q5** - Do we need other parameters here?\n",
    "> - **Answer:**\n",
    "    \n",
    "\n",
    "> **Q6** - How can we read a live video stream?\n",
    "> - **Answer:**\n",
    "\n",
    "\n",
    "**References:**\n",
    "- Source 3: https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef65d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first frame\n",
    "ret, prev_frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4499f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753e0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "print(prev_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a5894",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q7** - What does the `ret` parameter do here?\n",
    "> - **Answer:**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274c448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the frame to gray level\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbc53e",
   "metadata": {},
   "source": [
    "Define initial points to track (e.g., using `cv2.goodFeaturesToTrack`)\n",
    "- We can have any function which returns an array of the [x, y] coordination pairs here.\n",
    "\n",
    "Input Parameters:\n",
    "- `prev_gray`: the frame that used to extract feature points or anchor points from.\n",
    "- `maxCorners`: the maximum number of points to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f8898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_points = cv2.goodFeaturesToTrack(\n",
    "    prev_gray,\n",
    "    maxCorners=100,\n",
    "    qualityLevel=0.3,\n",
    "    minDistance=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4761f98",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q8** - What if changing `maxCorners` to `4`?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "> **Q9** - What if changing `qualityLevel` to `0.9` and `0.01`?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "> **Q10** - The `minDistance` parameter is the minimum distance between detected feature points in pixel. What if changing it to `1` and `50`?\n",
    "> - **Answer:** \n",
    "    \n",
    "\n",
    "**References:**\n",
    "- Source 4: https://docs.opencv.org/3.4/d4/d8c/tutorial_py_shi_tomasi.html \n",
    "- Source 5: https://theailearner.com/tag/cv2-goodfeaturestotrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd70f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    ret, curr_frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # track points using Lucas-Kanade method\n",
    "    prev_points, next_points = lucas_kanade_point_tracking(prev_gray, curr_gray, prev_points)\n",
    "\n",
    "    # draw tracks on the current frame\n",
    "    for i, (prev, next) in enumerate(zip(prev_points, next_points)):\n",
    "        x1, y1 = prev.ravel()\n",
    "        x2, y2 = next.ravel()\n",
    "        cv2.line(curr_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        cv2.circle(curr_frame, (int(x2), int(y2)), 5, (0, 255, 0), -1) \n",
    "        \n",
    "    cv2.namedWindow(\"Optical Flow\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('Optical Flow', curr_frame)\n",
    "    \n",
    "    # press 'Esc' to exit\n",
    "    if cv2.waitKey(0) & 0xFF == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d596a",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "> **Q11** - How can we break the `while` loop? why anded with `0xFF`?\n",
    "> - **Answer:** \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0677d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de9f34",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next:\n",
    "\n",
    "We have seen an example using video `10231.mp4`. You might want to explore other videos in this tutorial:\n",
    "\n",
    "- Example 1: `vtest.avi`\n",
    "- Example 2: `10142.mp4`\n",
    "- Example 3: `10231.mp4` (Completed)\n",
    "- Example 4: `10236.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e006d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b579d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40403c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1a8b11",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ Congratulations! You have finished this lab tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
